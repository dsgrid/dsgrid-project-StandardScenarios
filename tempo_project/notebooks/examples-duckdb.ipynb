{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3332ddda-0092-4936-ba58-729435adeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_dir = Path(\"/projects/dsgrid/tempo-project/april-queries/\")\n",
    "dataset_name = \"full_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441948f1-8c43-43cd-a42a-a53f2ab3213d",
   "metadata": {},
   "source": [
    "## DuckDB and Partitioned File Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44771550-acd4-4a39-a707-6c66270f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_partitioned(filepath):\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem) and (len(p.stem.split(\"=\")) == 2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_partitions(filepath):\n",
    "    assert is_partitioned(filepath), f\"{filepath} is not partitioned\"\n",
    "    \n",
    "    partition_name = None\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem):\n",
    "            tmp, value = p.stem.split(\"=\")\n",
    "            if partition_name:\n",
    "                assert (tmp == partition_name), f\"Found two different partition names in {filepath}: {partition_name}, {tmp}\"\n",
    "            partition_name = tmp\n",
    "            yield partition_name, value, p\n",
    "\n",
    "def print_partitions(filepath, print_depth=2, _depth=0):\n",
    "    if is_partitioned(filepath):\n",
    "        space = ' ' * 4 * _depth\n",
    "        for partition_name, value, p in get_partitions(filepath):\n",
    "            print(f\"{space}{partition_name}={value}\")\n",
    "        if (not print_depth) or ((_depth + 1) < print_depth):\n",
    "            print_partitions(p, print_depth=print_depth, _depth=_depth+1)\n",
    "\n",
    "def load_table(filepath, tablename):\n",
    "    duckdb.sql(f\"CREATE TABLE {tablename} AS SELECT * FROM read_parquet('{filepath}/**/*.parquet', hive_partitioning=true, hive_types_autocast=false);\")\n",
    "    description = duckdb.sql(f\"DESCRIBE {tablename};\")\n",
    "    logger.info(f\"Loaded {filepath} as {tablename}:\\n{description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd5979-14e6-4c9b-8cf1-5564c99299a4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb7e3c3-84cc-4cc9-a24a-aa9f05a7468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dataset_path):\n",
    "    with open(dataset_path / \"metadata.json\") as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "# load metadata and get column names by type\n",
    "metadata = get_metadata(data_dir / dataset_name)\n",
    "assert metadata[\"table_format\"][\"format_type\"] == \"unpivoted\", metadata[\"table_format\"]\n",
    "value_column = metadata[\"table_format\"][\"value_column\"]\n",
    "columns_by_type = {dim_type: metadata[\"dimensions\"][dim_type][0][\"column_names\"][0] for dim_type in metadata[\"dimensions\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2c18f-c24d-49c2-a864-46ec8cfd1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data table\n",
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "tablename = \"tbl\"\n",
    "load_table(filepath, tablename)\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793f47d-3c20-4a2a-b9bd-be83668fb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = duckdb.sql(f\"SELECT COUNT(*) as count FROM {tablename};\").df()[\"count\"].values[0]\n",
    "print(f\"Dataset contains {count:,} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b304-e335-4398-ae76-02d31c628b7f",
   "metadata": {},
   "source": [
    "## Recreate Lefthand Side of Figure ES-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad88239-6207-4f3a-96f5-7c7ee2ddbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.sql(f\"\"\"SELECT scenario, {columns_by_type[\"model_year\"]} as year, SUM({value_column})/1.0E6 as annual_twh\n",
    "                      FROM {tablename} \n",
    "                  GROUP BY scenario, {columns_by_type[\"model_year\"]}\n",
    "                  ORDER BY scenario, year\"\"\").df()\n",
    "df[\"scenario\"] = df[\"scenario\"].map({\n",
    "    \"efs_high_ldv\": \"EFS High Electrification\",\n",
    "    \"ldv_sales_evs_2035\": \"All LDV Sales EV by 2035\",\n",
    "    \"reference\": \"AEO Reference\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a264af-bd75-4034-81f5-fce40fb79f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df, x=\"year\", y=\"annual_twh\", color=\"scenario\", \n",
    "              labels={\"annual_twh\": \"EV Load (TWh/yr)\", \"scenario\": \"Scenario\"}, \n",
    "              range_y=[-25,1025],\n",
    "              width=600, height=450, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c4518-64af-40d9-815c-651ccf4c9a47",
   "metadata": {},
   "source": [
    "## Verify Timestamps Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc74c7-0d6b-4eea-be10-fe8ee1702534",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7598eb-c691-49f7-8220-5ce5cca72285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the data and look at initial timestamps\n",
    "\n",
    "where_clause = f\"(scenario = 'reference') AND ({columns_by_type['model_year']} = 2050)\"\n",
    "\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    where_clause += f\" AND ({columns_by_type['geography']} = 'middle_atlantic')\"\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    where_clause += f\" AND ({columns_by_type['geography']} = 'RI')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if columns_by_type['subsector'] == \"subsector\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'bev_compact')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} WHERE {where_clause} ORDER BY time_est LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba222b4-8125-4faf-a536-74683eea7b07",
   "metadata": {},
   "source": [
    "**Because the data type is \"TIMESTAMP\" and the first timestamp listed is \"2012-01-01 05:00:00\", we can see that DuckDB is importing time_est as timezone-unaware timestamps that are in UTC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bf45f-cd87-48fb-9783-44280839efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate how to shift timestamps to match 'time_est' label\n",
    "\n",
    "columns_clause = \"\"; sep= \"\"\n",
    "for column in duckdb.sql(f\"DESCRIBE {tablename};\").df()[\"column_name\"].tolist():\n",
    "    if column == \"time_est\":\n",
    "        columns_clause += sep + \"time_est - INTERVAL 5 HOUR as time_est\"\n",
    "    else:\n",
    "        columns_clause += sep + column\n",
    "    sep = \", \"\n",
    "\n",
    "sql_stmt = f\"\"\"\n",
    "    SELECT {columns_clause} \n",
    "      FROM {tablename} \n",
    "     WHERE {where_clause} \n",
    "  ORDER BY time_est \n",
    "     LIMIT 5;\"\"\"\n",
    "logger.info(f\"SQL for adjusting timestamps:\\n{sql_stmt}\")\n",
    "duckdb.sql(sql_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92f277-9287-4f01-8f0e-8d98b9927d66",
   "metadata": {},
   "source": [
    "## Verify that Profiles in Different Timezones Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d3a52-e0d6-4996-ad82-62a1fad0f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f7c3b-71b0-4cff-b18c-ec309071f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile(start_timestamp, end_timestamp, where_clause, \n",
    "                tablename=tablename, value_column=value_column, \n",
    "                normalize_profile=True, replace_timestamps=True):\n",
    "    df = duckdb.sql(f\"\"\"SELECT time_est, SUM({value_column}) as {value_column}\n",
    "                          FROM {tablename} \n",
    "                         WHERE {where_clause} AND (time_est >= TIMESTAMP '{start_timestamp}') AND (time_est <= TIMESTAMP '{end_timestamp}')\n",
    "                      GROUP BY time_est \n",
    "                      ORDER BY time_est;\"\"\").df()\n",
    "    if normalize_profile:\n",
    "        df[value_column] = df[value_column] / df[value_column].sum()\n",
    "    if replace_timestamps:\n",
    "        df[\"hour\"] = df.index.values\n",
    "        df = df[[\"hour\",value_column]]\n",
    "    return df\n",
    "\n",
    "where_clause = f\"(scenario = 'reference') AND ({columns_by_type['model_year']} = 2050)\"\n",
    "\n",
    "if columns_by_type['subsector'] == \"subsector\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'bev_compact')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "geographies = None\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    geographies = {\"ET\": \"middle_atlantic\", \"CT\": \"west_south_central\", \"MT\": \"mountain\", \"PT\": \"pacific\"}\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    geographies = {\"ET\": \"NC\", \"CT\": \"TX\", \"MT\": \"CO\", \"PT\": \"OR\"}\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "days = {\n",
    "    \"Standard Time\": (dt.datetime(2012, 2, 14, 5), dt.datetime(2012, 2, 15, 4)),        # Selects UTC timestamps that correspond to EST 2/14/2012\n",
    "    \"Daylight Savings Time\": (dt.datetime(2012, 8, 14, 5), dt.datetime(2012, 8, 15, 4)) # Selects UTC timestamps that correspond to EST 8/14/2012\n",
    "}\n",
    "\n",
    "data = []\n",
    "for time_type, time_tuple in days.items():\n",
    "    for tz, geo in geographies.items():\n",
    "        data.append(get_profile(time_tuple[0], time_tuple[1], where_clause + f\" AND {columns_by_type['geography']} = '{geo}'\"))\n",
    "        data[-1][\"Time Type\"] = time_type\n",
    "        data[-1][\"Time Zone\"] = tz\n",
    "df = pd.concat(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99436c-0d56-4630-b6d3-3cfdfa237696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df, x=\"hour\", y=value_column, color=\"Time Zone\", line_dash=\"Time Type\",\n",
    "              color_discrete_map={\"ET\": \"red\", \"CT\": \"orange\", \"MT\": \"blue\", \"PT\": \"purple\"},\n",
    "              labels={\"value\": \"Normalized Load Profile\", \"hour\": \"Hour of EST Day\"},\n",
    "              #range_y=[0,0.1],\n",
    "              width=600, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fe721-29a4-430d-991e-abfafabbd06a",
   "metadata": {},
   "source": [
    "## Demonstrate Loading a Subset of a Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cb19e6-d0f5-4f71-a907-e0f8a775e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario=efs_high_ldv\n",
      "scenario=ldv_sales_evs_2035\n",
      "scenario=reference\n",
      "    tempo_project_model_years=2024\n",
      "    tempo_project_model_years=2046\n",
      "    tempo_project_model_years=2030\n",
      "    tempo_project_model_years=2044\n",
      "    tempo_project_model_years=2026\n",
      "    tempo_project_model_years=2036\n",
      "    tempo_project_model_years=2042\n",
      "    tempo_project_model_years=2050\n",
      "    tempo_project_model_years=2028\n",
      "    tempo_project_model_years=2048\n",
      "    tempo_project_model_years=2040\n",
      "    tempo_project_model_years=2038\n",
      "    tempo_project_model_years=2032\n",
      "    tempo_project_model_years=2034\n",
      "        state=MA\n",
      "        state=KY\n",
      "        state=NH\n",
      "        state=MI\n",
      "        state=CT\n",
      "        state=NE\n",
      "        state=IL\n",
      "        state=MD\n",
      "        state=WY\n",
      "        state=RI\n",
      "        state=CA\n",
      "        state=MT\n",
      "        state=MN\n",
      "        state=TN\n",
      "        state=NJ\n",
      "        state=NC\n",
      "        state=ID\n",
      "        state=IN\n",
      "        state=ND\n",
      "        state=WV\n",
      "        state=WI\n",
      "        state=MS\n",
      "        state=CO\n",
      "        state=SC\n",
      "        state=KS\n",
      "        state=NY\n",
      "        state=FL\n",
      "        state=OK\n",
      "        state=VA\n",
      "        state=UT\n",
      "        state=NM\n",
      "        state=GA\n",
      "        state=MO\n",
      "        state=AZ\n",
      "        state=AR\n",
      "        state=DC\n",
      "        state=IA\n",
      "        state=OR\n",
      "        state=ME\n",
      "        state=PA\n",
      "        state=TX\n",
      "        state=AL\n",
      "        state=VT\n",
      "        state=OH\n",
      "        state=SD\n",
      "        state=NV\n",
      "        state=DE\n",
      "        state=LA\n",
      "        state=WA\n"
     ]
    }
   ],
   "source": [
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "print_partitions(filepath, print_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0046fa7-d77b-40b8-8267-120184bb7ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb860c06429f48488ea08a2b97ef0807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Edit this list of tuples as desired\n",
    "partitions=[\n",
    "    (\"scenario\", \"efs_high_ldv\"),\n",
    "    (\"tempo_project_model_years\", \"2050\")\n",
    "]\n",
    "\n",
    "subset_filepath = filepath\n",
    "for partition_name, value in partitions:\n",
    "    subset_filepath = subset_filepath / f\"{partition_name}={value}\"\n",
    "\n",
    "assert subset_filepath.exists(), f\"{subset_filepath} does not exist. Edit `partitions`.\"\n",
    "\n",
    "# load partial data table\n",
    "tablename = \"tbl\"\n",
    "load_table(subset_filepath, tablename)\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fc96b-66e7-437f-80c8-e2a1b9d410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = duckdb.sql(f\"SELECT COUNT(*) as count FROM {tablename};\").df()[\"count\"].values[0]\n",
    "print(f\"Dataset contains {count:,} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ee522-6844-4daf-b8c6-359c30488b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add delete, insert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
