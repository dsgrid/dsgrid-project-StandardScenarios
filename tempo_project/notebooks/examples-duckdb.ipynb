{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332ddda-0092-4936-ba58-729435adeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from s3pathlib import S3Path\n",
    "from pyarrow import fs\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194274e-8c00-40c4-8fcc-be20d4245372",
   "metadata": {},
   "source": [
    "## Setup data loading, local or s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901912c7-949d-4459-be61-1ac0d2e21156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data stream, either local filesys or s3\n",
    "dataset_name = \"state_level_simplified\"\n",
    "\n",
    "data_dir = Path(\"/datasets/dsgrid/dsgrid-tempo-v2022/\")\n",
    "use_s3 = False\n",
    "s3 = None\n",
    "\n",
    "if not data_dir.exists():\n",
    "    use_s3 = True\n",
    "    data_dir = S3Path(\"nrel-pds-dsgrid\", \"tempo/tempo-2022/v1.0.0/\")\n",
    "    s3 = fs.S3FileSystem(region=fs.resolve_s3_region('nrel-pds-dsgrid'))\n",
    "    \n",
    "# helper open fn:\n",
    "def open_handler(file_path):\n",
    "    if use_s3:\n",
    "        return s3.open_input_stream('/'.join((file_path.bucket, file_path.key)))\n",
    "    else:\n",
    "        return open(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441948f1-8c43-43cd-a42a-a53f2ab3213d",
   "metadata": {},
   "source": [
    "## DuckDB and Partitioned File Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44771550-acd4-4a39-a707-6c66270f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_partitioned(filepath):\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem) and (len(p.stem.split(\"=\")) == 2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_partitions(filepath):\n",
    "    assert is_partitioned(filepath), f\"{filepath} is not partitioned\"\n",
    "    \n",
    "    partition_name = None\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem):\n",
    "            tmp, value = p.stem.split(\"=\")\n",
    "            if partition_name:\n",
    "                assert (tmp == partition_name), f\"Found two different partition names in {filepath}: {partition_name}, {tmp}\"\n",
    "            partition_name = tmp\n",
    "            yield partition_name, value, p\n",
    "\n",
    "def print_partitions(filepath, print_depth=2, _depth=0):\n",
    "    if is_partitioned(filepath):\n",
    "        space = ' ' * 4 * _depth\n",
    "        for partition_name, value, p in get_partitions(filepath):\n",
    "            print(f\"{space}{partition_name}={value}\")\n",
    "        if (not print_depth) or ((_depth + 1) < print_depth):\n",
    "            print_partitions(p, print_depth=print_depth, _depth=_depth+1)\n",
    "\n",
    "def table_exists(tablename):\n",
    "    return tablename in duckdb.sql(\"SHOW TABLES\").df()[\"name\"].tolist()\n",
    "\n",
    "def drop_table(tablename):\n",
    "    duckdb.sql(f\"DROP TABLE {tablename}\")\n",
    "\n",
    "def load_table(filepath, tablename):    \n",
    "    if table_exists(tablename):\n",
    "        logger.warning(f\"Dropping and replacing table {tablename!r}.\")\n",
    "        drop_table(tablename)\n",
    "    filepath = filepath.uri if use_s3 else filepath\n",
    "        \n",
    "    duckdb.sql(f\"\"\"CREATE TABLE {tablename} AS SELECT * \n",
    "                     FROM read_parquet('{filepath}/**/*.parquet', hive_partitioning=true, hive_types_autocast=false)\"\"\")\n",
    "    description = duckdb.sql(f\"DESCRIBE {tablename}\")\n",
    "    logger.info(f\"Loaded {filepath} as {tablename}:\\n{description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd5979-14e6-4c9b-8cf1-5564c99299a4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7e3c3-84cc-4cc9-a24a-aa9f05a7468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dataset_path):\n",
    "    with open_handler(dataset_path / \"metadata.json\") as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "# load metadata and get column names by type\n",
    "metadata = get_metadata(data_dir / dataset_name)\n",
    "assert metadata[\"table_format\"][\"format_type\"] == \"unpivoted\", metadata[\"table_format\"]\n",
    "value_column = metadata[\"table_format\"][\"value_column\"]\n",
    "columns_by_type = {dim_type: metadata[\"dimensions\"][dim_type][0][\"column_names\"][0] \n",
    "                   for dim_type in metadata[\"dimensions\"] if metadata[\"dimensions\"][dim_type]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2c18f-c24d-49c2-a864-46ec8cfd1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data table\n",
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "tablename = \"tbl\"\n",
    "load_table(filepath, tablename)\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793f47d-3c20-4a2a-b9bd-be83668fb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = duckdb.sql(f\"SELECT COUNT(*) as count FROM {tablename}\").df()[\"count\"].values[0]\n",
    "print(f\"Dataset contains {count:,} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b304-e335-4398-ae76-02d31c628b7f",
   "metadata": {},
   "source": [
    "## Recreate Lefthand Side of Figure ES-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad88239-6207-4f3a-96f5-7c7ee2ddbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.sql(f\"\"\"SELECT scenario, {columns_by_type[\"model_year\"]} as year, \n",
    "                           SUM({value_column})/1.0E6 as annual_twh\n",
    "                      FROM {tablename} \n",
    "                  GROUP BY scenario, {columns_by_type[\"model_year\"]}\n",
    "                  ORDER BY scenario, year\"\"\").df()\n",
    "df[\"scenario\"] = df[\"scenario\"].map({\n",
    "    \"efs_high_ldv\": \"EFS High Electrification\",\n",
    "    \"ldv_sales_evs_2035\": \"All LDV Sales EV by 2035\",\n",
    "    \"reference\": \"AEO Reference\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a264af-bd75-4034-81f5-fce40fb79f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df, x=\"year\", y=\"annual_twh\", color=\"scenario\", \n",
    "              labels={\"annual_twh\": \"EV Load (TWh/yr)\", \"scenario\": \"Scenario\"}, \n",
    "              range_y=[-25,1025],\n",
    "              width=600, height=450, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c4518-64af-40d9-815c-651ccf4c9a47",
   "metadata": {},
   "source": [
    "## Verify Timestamps Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc74c7-0d6b-4eea-be10-fe8ee1702534",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7598eb-c691-49f7-8220-5ce5cca72285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the data and look at initial timestamps\n",
    "\n",
    "where_clause = f\"(scenario = 'reference') AND ({columns_by_type['model_year']} = 2050)\"\n",
    "\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    where_clause += f\" AND ({columns_by_type['geography']} = 'middle_atlantic')\"\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    where_clause += f\" AND ({columns_by_type['geography']} = 'RI')\"\n",
    "elif columns_by_type['geography'] == \"county\":\n",
    "    where_clause += f\" AND ({columns_by_type['geography']} = '39023')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if \"subsector\" not in columns_by_type:\n",
    "    pass\n",
    "elif columns_by_type['subsector'] == \"subsector\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'bev_compact')\"\n",
    "elif columns_by_type['subsector'] == \"household_and_vehicle_type\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'Some_Drivers_Larger+Low_Income+Second_City+Pickup+BEV_100')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if columns_by_type['metric'] == \"end_uses_by_fuel_type\":\n",
    "    pass\n",
    "elif columns_by_type['metric'] == \"end_use\":\n",
    "    where_clause += f\" AND ({columns_by_type['metric']} = 'electricity_ev_l1l2')\"   \n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} WHERE {where_clause} ORDER BY time_est LIMIT 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba222b4-8125-4faf-a536-74683eea7b07",
   "metadata": {},
   "source": [
    "**Because the data type is \"TIMESTAMP\" and the first timestamp listed is \"2012-01-01 05:00:00\", we can see that DuckDB is importing time_est as timezone-unaware timestamps that are in UTC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bf45f-cd87-48fb-9783-44280839efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate how to shift timestamps to match 'time_est' label\n",
    "\n",
    "columns_clause = \"\"; sep= \"\"\n",
    "for column in duckdb.sql(f\"DESCRIBE {tablename}\").df()[\"column_name\"].tolist():\n",
    "    if column == \"time_est\":\n",
    "        columns_clause += sep + \"time_est - INTERVAL 5 HOUR as time_est\"\n",
    "    else:\n",
    "        columns_clause += sep + column\n",
    "    sep = \", \"\n",
    "\n",
    "sql_stmt = f\"\"\"\n",
    "    SELECT {columns_clause} \n",
    "      FROM {tablename} \n",
    "     WHERE {where_clause} \n",
    "  ORDER BY time_est \n",
    "     LIMIT 5\"\"\"\n",
    "logger.info(f\"SQL for adjusting timestamps:\\n{sql_stmt}\")\n",
    "duckdb.sql(sql_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92f277-9287-4f01-8f0e-8d98b9927d66",
   "metadata": {},
   "source": [
    "## Verify that Profiles in Different Timezones Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d3a52-e0d6-4996-ad82-62a1fad0f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f7c3b-71b0-4cff-b18c-ec309071f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile(start_timestamp, end_timestamp, where_clause, \n",
    "                tablename=tablename, value_column=value_column, \n",
    "                normalize_profile=True, replace_timestamps=True):\n",
    "    df = duckdb.sql(f\"\"\"SELECT time_est, \n",
    "                               SUM({value_column}) as {value_column}\n",
    "                          FROM {tablename} \n",
    "                         WHERE {where_clause} AND \n",
    "                               (time_est >= TIMESTAMP '{start_timestamp}') AND \n",
    "                               (time_est <= TIMESTAMP '{end_timestamp}')\n",
    "                      GROUP BY time_est \n",
    "                      ORDER BY time_est\"\"\").df()\n",
    "    if normalize_profile:\n",
    "        df[value_column] = df[value_column] / df[value_column].sum()\n",
    "    if replace_timestamps:\n",
    "        df[\"hour\"] = df.index.values\n",
    "        df = df[[\"hour\",value_column]]\n",
    "    return df\n",
    "\n",
    "where_clause = f\"(scenario = 'reference') AND ({columns_by_type['model_year']} = 2050)\"\n",
    "\n",
    "if \"subsector\" not in columns_by_type:\n",
    "    pass\n",
    "elif columns_by_type['subsector'] == \"subsector\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'bev_compact')\"\n",
    "elif columns_by_type['subsector'] == \"household_and_vehicle_type\":\n",
    "    where_clause += f\" AND ({columns_by_type['subsector']} = 'Some_Drivers_Smaller+Middle_Income+Suburban+SUV+BEV_300')\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "geographies = None\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    geographies = {\"ET\": \"middle_atlantic\", \"CT\": \"west_south_central\", \"MT\": \"mountain\", \"PT\": \"pacific\"}\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    geographies = {\"ET\": \"NC\", \"CT\": \"TX\", \"MT\": \"CO\", \"PT\": \"OR\"}\n",
    "elif columns_by_type['geography'] == \"county\":\n",
    "    geographies = {\"ET\": \"37183\", \"CT\": \"48453\", \"MT\": \"08069\", \"PT\": \"06059\"}\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "days = {\n",
    "    \"Standard Time\": (dt.datetime(2012, 2, 14, 5), dt.datetime(2012, 2, 15, 4)),        # Selects UTC timestamps that correspond to EST 2/14/2012\n",
    "    \"Daylight Savings Time\": (dt.datetime(2012, 8, 14, 5), dt.datetime(2012, 8, 15, 4)) # Selects UTC timestamps that correspond to EST 8/14/2012\n",
    "}\n",
    "\n",
    "data = []\n",
    "for time_type, time_tuple in days.items():\n",
    "    for tz, geo in geographies.items():\n",
    "        data.append(get_profile(time_tuple[0], time_tuple[1], where_clause + f\" AND {columns_by_type['geography']} = '{geo}'\"))\n",
    "        data[-1][\"Time Type\"] = time_type\n",
    "        data[-1][\"Time Zone\"] = tz\n",
    "df = pd.concat(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99436c-0d56-4630-b6d3-3cfdfa237696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df, x=\"hour\", y=value_column, color=\"Time Zone\", line_dash=\"Time Type\",\n",
    "              color_discrete_map={\"ET\": \"red\", \"CT\": \"orange\", \"MT\": \"blue\", \"PT\": \"purple\"},\n",
    "              labels={\"value\": \"Normalized Load Profile\", \"hour\": \"Hour of EST Day\"},\n",
    "              #range_y=[0,0.1],\n",
    "              width=600, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fe721-29a4-430d-991e-abfafabbd06a",
   "metadata": {},
   "source": [
    "## Demonstrate Loading a Subset of a Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb19e6-d0f5-4f71-a907-e0f8a775e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "print_partitions(filepath, print_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0046fa7-d77b-40b8-8267-120184bb7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this list of tuples as desired\n",
    "partitions=[\n",
    "    (\"scenario\", \"efs_high_ldv\")\n",
    "]\n",
    "\n",
    "subset_filepath = filepath\n",
    "for partition_name, value in partitions:\n",
    "    subset_filepath = subset_filepath / f\"{partition_name}={value}\"\n",
    "\n",
    "assert subset_filepath.exists(), f\"{subset_filepath} does not exist. Edit `partitions`.\"\n",
    "\n",
    "# load partial data table\n",
    "tablename = \"tbl\"\n",
    "load_table(subset_filepath, tablename)\n",
    "duckdb.sql(f\"SELECT * FROM {tablename} LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fc96b-66e7-437f-80c8-e2a1b9d410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = duckdb.sql(f\"SELECT COUNT(*) as count FROM {tablename}\").df()[\"count\"].values[0]\n",
    "print(f\"Dataset contains {count:,} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1567f-a149-4676-92e3-a745622c8888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
