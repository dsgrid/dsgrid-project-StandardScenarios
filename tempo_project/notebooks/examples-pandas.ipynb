{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70fa67-59da-4d90-b653-6e4a9b2e09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from s3pathlib import S3Path\n",
    "from pyarrow import fs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0afd92-6a48-4038-a518-ddaaa5b87cdc",
   "metadata": {},
   "source": [
    "## Setup data loading, local or s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81e063-f61b-4f0f-8736-2c59bcc48dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data stream, either local filesys or s3\n",
    "dataset_name = \"state_level_simplified\"\n",
    "\n",
    "data_dir = Path(\"/datasets/dsgrid/dsgrid-tempo-v2022/\")\n",
    "use_s3 = False\n",
    "\n",
    "if not data_dir.exists():\n",
    "    use_s3 = True\n",
    "    data_dir = S3Path(\"nrel-pds-dsgrid\", \"tempo/tempo-2022/v1.0.0/\")\n",
    "    s3 = fs.S3FileSystem(region=fs.resolve_s3_region('nrel-pds-dsgrid'))\n",
    "    \n",
    "# helper open fn:\n",
    "def open_handler(file_path):\n",
    "    if use_s3:\n",
    "        return s3.open_input_stream('/'.join((file_path.bucket, file_path.key)))\n",
    "    else:\n",
    "        return open(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423002cd-ec06-4624-bba2-70cec712d28b",
   "metadata": {},
   "source": [
    "## Partitioned File Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c157b6d-74d0-45c5-9325-40e49b7eac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_partitioned(filepath):\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem) and (len(p.stem.split(\"=\")) == 2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_partitions(filepath):\n",
    "    assert is_partitioned(filepath), f\"{filepath} is not partitioned\"\n",
    "    \n",
    "    partition_name = None\n",
    "    for p in filepath.iterdir():\n",
    "        if p.is_dir() and (\"=\" in p.stem):\n",
    "            tmp, value = p.stem.split(\"=\")\n",
    "            if partition_name:\n",
    "                assert (tmp == partition_name), f\"Found two different partition names in {filepath}: {partition_name}, {tmp}\"\n",
    "            partition_name = tmp\n",
    "            yield partition_name, value, p\n",
    "\n",
    "def print_partitions(filepath, print_depth=2, _depth=0):\n",
    "    if is_partitioned(filepath):\n",
    "        space = ' ' * 4 * _depth\n",
    "        for partition_name, value, p in get_partitions(filepath):\n",
    "            print(f\"{space}{partition_name}={value}\")\n",
    "        if (not print_depth) or ((_depth + 1) < print_depth):\n",
    "            print_partitions(p, print_depth=print_depth, _depth=_depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f7b59-75fc-43c7-bc2d-43287591712e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2850f78-a0b7-44b2-bfd2-3962f7b1f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dataset_path):\n",
    "    with open_handler(dataset_path / \"metadata.json\") as f:\n",
    "        result = json.load(f)\n",
    "    return result\n",
    "\n",
    "# load metadata and get column names by type\n",
    "metadata = get_metadata(data_dir / dataset_name)\n",
    "assert metadata[\"table_format\"][\"format_type\"] == \"unpivoted\", metadata[\"table_format\"]\n",
    "value_column = metadata[\"table_format\"][\"value_column\"]\n",
    "columns_by_type = {dim_type: metadata[\"dimensions\"][dim_type][0][\"column_names\"][0] \n",
    "                   for dim_type in metadata[\"dimensions\"] if metadata[\"dimensions\"][dim_type]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acf46e-3173-4157-8060-e5679818f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data table\n",
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "\n",
    "filestr = filepath.uri if use_s3 else str(filepath)\n",
    "\n",
    "df = pd.read_parquet(filestr)\n",
    "    \n",
    "logger.info(f\"df.dtypes = \\n{df.dtypes}\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc92a26-d386-44b6-8143-c1917d4cdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains {len(df.index):,} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572da27-bd2c-40da-a88d-e778d21d01c1",
   "metadata": {},
   "source": [
    "## Recreate Lefthand Side of Figure ES-1Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e934244-95d4-4021-b130-4d98681a604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (df.groupby([\"scenario\", columns_by_type[\"model_year\"]])[value_column].sum() / 1.0E6).reset_index()\n",
    "df2.rename({columns_by_type[\"model_year\"]: \"year\", value_column: \"annual_twh\"}, axis=1, inplace=True)\n",
    "df2[\"scenario\"] = df2[\"scenario\"].map({\n",
    "    \"efs_high_ldv\": \"EFS High Electrification\",\n",
    "    \"ldv_sales_evs_2035\": \"All LDV Sales EV by 2035\",\n",
    "    \"reference\": \"AEO Reference\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe858d48-7e61-4692-9f23-531eebe2aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df2, x=\"year\", y=\"annual_twh\", color=\"scenario\", \n",
    "              labels={\"annual_twh\": \"EV Load (TWh/yr)\", \"scenario\": \"Scenario\"}, \n",
    "              range_y=[-25,1025],\n",
    "              width=600, height=450, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b95ba-6735-4476-a49d-5fbdcc7f23d4",
   "metadata": {},
   "source": [
    "## Verify Timestamps Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99c114-4f8c-4da7-8b61-703f6b41f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828ea48-6a5a-4da6-aa75-f0c5cf043ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the data and look at initial timestamps\n",
    "\n",
    "inds = (df[\"scenario\"] == 'reference') & (df[columns_by_type['model_year']] == '2050')\n",
    "\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    inds &= (df[columns_by_type['geography']] == 'middle_atlantic')\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    inds &= (df[columns_by_type['geography']] == 'RI')\n",
    "elif columns_by_type['geography'] == \"county\":\n",
    "    inds &= (df[columns_by_type['geography']] == '39023')\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if \"subsector\" not in columns_by_type:\n",
    "    pass\n",
    "elif columns_by_type['subsector'] == \"subsector\":\n",
    "    inds &= (df[columns_by_type['subsector']] == 'bev_compact')\n",
    "elif columns_by_type['subsector'] == \"household_and_vehicle_type\":\n",
    "    inds &= (df[columns_by_type['subsector']] == 'Some_Drivers_Larger+Low_Income+Second_City+Pickup+BEV_100')\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if columns_by_type['metric'] == \"end_uses_by_fuel_type\":\n",
    "    pass\n",
    "elif columns_by_type['metric'] == \"end_use\":\n",
    "    inds &= (df[columns_by_type['metric']] == 'electricity_ev_l1l2')\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "df2 = df[inds].sort_values(\"time_est\")\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fff2da-ddea-4f0f-a155-953ab02052af",
   "metadata": {},
   "source": [
    "**When loading from .parquet, the first timestamp listed is \"2012-01-01 05:00:00\". Thus Pandas imports time_est as timezone-unaware timestamps in UTC in this case.**\n",
    "\n",
    "**When loading from .csv with kwarg parse_dates=[\"time_est\"], the first timestamp listed is \"2012-01-01 00:00:00\" such that no further transformation is needed to show the time in EST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fea7d-45c0-4aad-9c43-5f1293e1c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate how to shift timestamps to match 'time_est' label\n",
    "\n",
    "df3 = df2.copy()\n",
    "df3[\"time_est\"] = df3[\"time_est\"] - dt.timedelta(hours=5)\n",
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fb042-7210-4692-b8a3-f6676fe14c3e",
   "metadata": {},
   "source": [
    "## Verify that Profiles in Different Timezones Are As Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc373c3-9815-49ae-9cda-9f30d54eb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert columns_by_type[\"time\"] == \"time_est\", \"Code in this section only makes sense if the dataset has timestamps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e89d69-007d-47f4-a717-05da1d887523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile(start_timestamp, end_timestamp, inds, value_column=value_column, \n",
    "                normalize_profile=True, replace_timestamps=True):\n",
    "    \n",
    "    inds &= ((df['time_est'] >= start_timestamp) & (df['time_est'] <= end_timestamp))\n",
    "    df2 = df[inds].groupby(\"time_est\")[value_column].sum().reset_index().sort_values(\"time_est\")\n",
    "    \n",
    "    if normalize_profile:\n",
    "        df2[value_column] = df2[value_column] / df2[value_column].sum()\n",
    "    if replace_timestamps:\n",
    "        df2[\"hour\"] = df2.index.values\n",
    "        df2 = df2[[\"hour\",value_column]]\n",
    "    return df2\n",
    "\n",
    "inds = (df[\"scenario\"] == 'reference') & (df[columns_by_type['model_year']] == '2050')\n",
    "\n",
    "if \"subsector\" not in columns_by_type:\n",
    "    pass\n",
    "elif columns_by_type['subsector'] == \"subsector\":\n",
    "    inds &= (df[columns_by_type['subsector']] == 'bev_compact')\n",
    "elif columns_by_type['subsector'] == \"household_and_vehicle_type\":\n",
    "    inds &= (df[columns_by_type['subsector']] == 'Some_Drivers_Smaller+Middle_Income+Suburban+SUV+BEV_300')\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "geographies = None\n",
    "if columns_by_type['geography'] == \"census_division\":\n",
    "    geographies = {\"ET\": \"middle_atlantic\", \"CT\": \"west_south_central\", \"MT\": \"mountain\", \"PT\": \"pacific\"}\n",
    "elif columns_by_type['geography'] == \"state\":\n",
    "    geographies = {\"ET\": \"NC\", \"CT\": \"TX\", \"MT\": \"CO\", \"PT\": \"OR\"}\n",
    "elif columns_by_type['geography'] == \"county\":\n",
    "    geographies = {\"ET\": \"37183\", \"CT\": \"48453\", \"MT\": \"08069\", \"PT\": \"06059\"}\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "days = {\n",
    "    \"Standard Time\": (dt.datetime(2012, 2, 14, 5), dt.datetime(2012, 2, 15, 4)),        # Selects UTC timestamps that correspond to EST 2/14/2012\n",
    "    \"Daylight Savings Time\": (dt.datetime(2012, 8, 14, 5), dt.datetime(2012, 8, 15, 4)) # Selects UTC timestamps that correspond to EST 8/14/2012\n",
    "}\n",
    "\n",
    "data = []\n",
    "for time_type, time_tuple in days.items():\n",
    "    for tz, geo in geographies.items():\n",
    "        data.append(get_profile(time_tuple[0], time_tuple[1], inds & (df[columns_by_type['geography']] == geo)))\n",
    "        data[-1][\"Time Type\"] = time_type\n",
    "        data[-1][\"Time Zone\"] = tz\n",
    "df2 = pd.concat(data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee273a-8fef-4b2f-829d-2bc68e6a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df2, x=\"hour\", y=value_column, color=\"Time Zone\", line_dash=\"Time Type\",\n",
    "              color_discrete_map={\"ET\": \"red\", \"CT\": \"orange\", \"MT\": \"blue\", \"PT\": \"purple\"},\n",
    "              labels={\"value\": \"Normalized Load Profile\", \"hour\": \"Hour of EST Day\"},\n",
    "              #range_y=[0,0.1],\n",
    "              width=600, template=\"plotly_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835600f-3c5a-4670-93a7-db50ab3f536e",
   "metadata": {},
   "source": [
    "## Demonstrate Loading a Subset of a Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b14a34-77c1-478e-a4e0-c402f87aad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = data_dir / dataset_name / \"table.parquet\"\n",
    "print_partitions(filepath, print_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2c87d-4e2c-4252-a41f-363d7d90d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this list of tuples as desired\n",
    "partitions=[\n",
    "    (\"scenario\", \"efs_high_ldv\")\n",
    "]\n",
    "\n",
    "subset_filepath = filepath\n",
    "for partition_name, value in partitions:\n",
    "    subset_filepath = subset_filepath / f\"{partition_name}={value}\"\n",
    "\n",
    "# load partial data table\n",
    "df = pd.read_parquet(subset_filepath)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df2b19-c2a7-4d5a-a380-ea6a1f5cc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains {len(df.index):,} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547529a-2900-48ab-bd8f-6eb7a6919a72",
   "metadata": {},
   "source": [
    "## Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f203b9c-51db-4813-adcd-89003aa1b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV's can be found in some of the aggregate summary datasets\n",
    "filepath = data_dir / \"annual_summary_state\" / \"table.csv\"\n",
    "filestr = filepath.uri if use_s3 else str(filepath)\n",
    "\n",
    "kwargs = { \n",
    "    \"dtype\": { columns_by_type['model_year']: str }\n",
    "}        \n",
    "df = pd.read_csv(filestr, **kwargs)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
